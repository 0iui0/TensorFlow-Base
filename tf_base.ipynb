{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_base.ipynb",
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEWOdk0U5Jt_"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JJ0E9Aw5aP0"
      },
      "source": [
        "a=tf.constant([1,2,3,1,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JEMhJrs5ll9"
      },
      "source": [
        "b=tf.constant([0,1,3,4,5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cnc4z-R954hl"
      },
      "source": [
        "tf.greater(a,b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy0A5_715q7I"
      },
      "source": [
        "tf.where(tf.greater(a,b),a,b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UugMx4H85-gz"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvPJm5_16WbL"
      },
      "source": [
        "rdm=np.random.RandomState(seed=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCQYu4BP6dv2"
      },
      "source": [
        "rdm.rand()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uznRnrry6-of"
      },
      "source": [
        "rdm.rand(2,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMLEhzyY7H36"
      },
      "source": [
        "np.vstack((a,b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMM3rXYJ7Zto"
      },
      "source": [
        "x,y=np.mgrid[1:4:1,2:4:0.5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGo6-5bP8sXp"
      },
      "source": [
        "x,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h7N27kJ72eD"
      },
      "source": [
        "x.ravel(),y.ravel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtaapzdT7_q9"
      },
      "source": [
        "np.c_[x.ravel(),y.ravel()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx_vA9ZvCZNm"
      },
      "source": [
        "import sympy as sp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2cwsQI3CcrA"
      },
      "source": [
        "x=sp.Symbol('x')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvaMOy6cChmW"
      },
      "source": [
        "y=x*x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uXH2rOiDLO4"
      },
      "source": [
        "y.diff(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ6D0HehFKw4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8ws0exbDyO3"
      },
      "source": [
        "tf.nn.sigmoid(0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7QYMtTuFIoD"
      },
      "source": [
        "SEED=23455"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnYYecF7hsCb"
      },
      "source": [
        "rdm=np.random.RandomState(seed=SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJkke2Yxh0nW"
      },
      "source": [
        "x=rdm.rand(32,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCd7BUNgh-Kr"
      },
      "source": [
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtaKj3J3iMVP"
      },
      "source": [
        "y_=[[x1+x2+rdm.rand()/10-0.05] for (x1,x2) in x]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9utSVCgilSB"
      },
      "source": [
        "y_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnHkRrZYimjv"
      },
      "source": [
        "x=tf.cast(x,dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgHcjxeCi4zz"
      },
      "source": [
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S76IqMOxi5eT"
      },
      "source": [
        "w1=tf.Variable(tf.random.normal([2,1],stddev=1.0,seed=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-dt1tJIjY0j"
      },
      "source": [
        "w1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOnfKHEkjZbb"
      },
      "source": [
        "epoch=15000\n",
        "lr=0.002"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9O8kLhQjs68"
      },
      "source": [
        "for epoch in range(epoch):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y=tf.matmul(x,w1)\n",
        "    # loss=tf.reduce_mean(tf.square(y-y_))\n",
        "    loss=tf.reduce_sum(tf.where(tf.greater(y,y_),(y-y_)*10,(y_-y)*9))\n",
        "  grads=tape.gradient(loss,w1)\n",
        "  w1.assign_sub(lr*grads)\n",
        "\n",
        "  if epoch%500==0:\n",
        "    print(\"epoch %d, w1 is \"%(epoch))\n",
        "    print(w1.numpy(),\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNiISQ4vlNUP"
      },
      "source": [
        "w1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPGK3G0jnFgK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsETrlFYqOwg"
      },
      "source": [
        "cross_entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmI6WQBMqUEh"
      },
      "source": [
        "tf.losses.categorical_crossentropy([1,0],[0.6,0.4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOVnqC_HqoUy"
      },
      "source": [
        "tf.losses.categorical_crossentropy([1,0],[0.5,0.5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xXM-ixhsPfp"
      },
      "source": [
        "print(tf.__version__)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMvDhjJbuIhP"
      },
      "source": [
        "y=np.array([[12,13,5],[3,20,2]],dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlKpy_hnqxVz"
      },
      "source": [
        "y_pro=tf.nn.softmax(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbzTykXArXJj"
      },
      "source": [
        "y_s=np.array([[1,0,0],[0,1,0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHONe4H5uETE"
      },
      "source": [
        "tf.losses.categorical_crossentropy(y_s,y_pro)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGVq5rkYuVsY"
      },
      "source": [
        "tf.nn.softmax_cross_entropy_with_logits(y_s,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Mj2nWZqjE9J"
      },
      "source": [
        "# 利用鸢尾花数据集，实现前向传播、反向传播，可视化loss曲线\n",
        "\n",
        "# 导入所需模块\n",
        "import tensorflow as tf\n",
        "from sklearn import datasets\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import time  ##1##\n",
        "\n",
        "# 导入数据，分别为输入特征和标签\n",
        "x_data = datasets.load_iris().data\n",
        "y_data = datasets.load_iris().target\n",
        "\n",
        "# 随机打乱数据（因为原始数据是顺序的，顺序不打乱会影响准确率）\n",
        "# seed: 随机数种子，是一个整数，当设置之后，每次生成的随机数都一样（为方便教学，以保每位同学结果一致）\n",
        "np.random.seed(116)  # 使用相同的seed，保证输入特征和标签一一对应\n",
        "np.random.shuffle(x_data)\n",
        "np.random.seed(116)\n",
        "np.random.shuffle(y_data)\n",
        "tf.random.set_seed(116)\n",
        "\n",
        "# 将打乱后的数据集分割为训练集和测试集，训练集为前120行，测试集为后30行\n",
        "x_train = x_data[:-30]\n",
        "y_train = y_data[:-30]\n",
        "x_test = x_data[-30:]\n",
        "y_test = y_data[-30:]\n",
        "\n",
        "# 转换x的数据类型，否则后面矩阵相乘时会因数据类型不一致报错\n",
        "x_train = tf.cast(x_train, tf.float32)\n",
        "x_test = tf.cast(x_test, tf.float32)\n",
        "\n",
        "# from_tensor_slices函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据）\n",
        "train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
        "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
        "\n",
        "# 生成神经网络的参数，4个输入特征故，输入层为4个输入节点；因为3分类，故输出层为3个神经元\n",
        "# 用tf.Variable()标记参数可训练\n",
        "# 使用seed使每次生成的随机数相同（方便教学，使大家结果都一致，在现实使用时不写seed）\n",
        "w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev=0.1, seed=1))\n",
        "b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1))\n",
        "\n",
        "lr = 0.1  # 学习率为0.1\n",
        "train_loss_results = []  # 将每轮的loss记录在此列表中，为后续画loss曲线提供数据\n",
        "test_acc = []  # 将每轮的acc记录在此列表中，为后续画acc曲线提供数据\n",
        "epoch = 500  # 循环500轮\n",
        "loss_all = 0  # 每轮分4个step，loss_all记录四个step生成的4个loss的和\n",
        "\n",
        "# 训练部分\n",
        "now_time = time.time()  ##2##\n",
        "for epoch in range(epoch):  # 数据集级别的循环，每个epoch循环一次数据集\n",
        "    for step, (x_train, y_train) in enumerate(train_db):  # batch级别的循环 ，每个step循环一个batch\n",
        "        with tf.GradientTape() as tape:  # with结构记录梯度信息\n",
        "            y = tf.matmul(x_train, w1) + b1  # 神经网络乘加运算\n",
        "            y = tf.nn.softmax(y)  # 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）\n",
        "            y_ = tf.one_hot(y_train, depth=3)  # 将标签值转换为独热码格式，方便计算loss和accuracy\n",
        "            loss = tf.reduce_mean(tf.square(y_ - y))  # 采用均方误差损失函数mse = mean(sum(y-out)^2)\n",
        "            loss_all += loss.numpy()  # 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确\n",
        "        # 计算loss对各个参数的梯度\n",
        "        grads = tape.gradient(loss, [w1, b1])\n",
        "\n",
        "        # 实现梯度更新 w1 = w1 - lr * w1_grad    b = b - lr * b_grad\n",
        "        w1.assign_sub(lr * grads[0])  # 参数w1自更新\n",
        "        b1.assign_sub(lr * grads[1])  # 参数b自更新\n",
        "\n",
        "    # 每个epoch，打印loss信息\n",
        "    print(\"Epoch {}, loss: {}\".format(epoch, loss_all / 4))\n",
        "    train_loss_results.append(loss_all / 4)  # 将4个step的loss求平均记录在此变量中\n",
        "    loss_all = 0  # loss_all归零，为记录下一个epoch的loss做准备\n",
        "\n",
        "    # 测试部分\n",
        "    # total_correct为预测对的样本个数, total_number为测试的总样本数，将这两个变量都初始化为0\n",
        "    total_correct, total_number = 0, 0\n",
        "    for x_test, y_test in test_db:\n",
        "        # 使用更新后的参数进行预测\n",
        "        y = tf.matmul(x_test, w1) + b1\n",
        "        y = tf.nn.softmax(y)\n",
        "        pred = tf.argmax(y, axis=1)  # 返回y中最大值的索引，即预测的分类\n",
        "        # 将pred转换为y_test的数据类型\n",
        "        pred = tf.cast(pred, dtype=y_test.dtype)\n",
        "        # 若分类正确，则correct=1，否则为0，将bool型的结果转换为int型\n",
        "        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)\n",
        "        # 将每个batch的correct数加起来\n",
        "        correct = tf.reduce_sum(correct)\n",
        "        # 将所有batch中的correct数加起来\n",
        "        total_correct += int(correct)\n",
        "        # total_number为测试的总样本数，也就是x_test的行数，shape[0]返回变量的行数\n",
        "        total_number += x_test.shape[0]\n",
        "    # 总的准确率等于total_correct/total_number\n",
        "    acc = total_correct / total_number\n",
        "    test_acc.append(acc)\n",
        "    print(\"Test_acc:\", acc)\n",
        "    print(\"--------------------------\")\n",
        "total_time = time.time() - now_time  ##3##\n",
        "print(\"total_time\", total_time)  ##4##\n",
        "\n",
        "# 绘制 loss 曲线\n",
        "plt.title('Loss Function Curve')  # 图片标题\n",
        "plt.xlabel('Epoch')  # x轴变量名称\n",
        "plt.ylabel('Loss')  # y轴变量名称\n",
        "plt.plot(train_loss_results, label=\"$Loss$\")  # 逐点画出trian_loss_results值并连线，连线图标是Loss\n",
        "plt.legend()  # 画出曲线图标\n",
        "plt.show()  # 画出图像\n",
        "\n",
        "# 绘制 Accuracy 曲线\n",
        "plt.title('Acc Curve')  # 图片标题\n",
        "plt.xlabel('Epoch')  # x轴变量名称\n",
        "plt.ylabel('Acc')  # y轴变量名称\n",
        "plt.plot(test_acc, label=\"$Accuracy$\")  # 逐点画出test_acc值并连线，连线图标是Accuracy\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 本文件较 class1\\p45_iris.py 仅添加四处时间记录  用 ##n## 标识\n",
        "# 请将loss曲线、ACC曲线、total_time记录到 class2\\优化器对比.docx  对比各优化器收敛情况\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54RIQ6rlnMw6"
      },
      "source": [
        "# 利用鸢尾花数据集，实现前向传播、反向传播，可视化loss曲线\n",
        "\n",
        "# 导入所需模块\n",
        "import tensorflow as tf\n",
        "from sklearn import datasets\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import time  ##1##\n",
        "\n",
        "# 导入数据，分别为输入特征和标签\n",
        "x_data = datasets.load_iris().data\n",
        "y_data = datasets.load_iris().target\n",
        "\n",
        "# 随机打乱数据（因为原始数据是顺序的，顺序不打乱会影响准确率）\n",
        "# seed: 随机数种子，是一个整数，当设置之后，每次生成的随机数都一样（为方便教学，以保每位同学结果一致）\n",
        "np.random.seed(116)  # 使用相同的seed，保证输入特征和标签一一对应\n",
        "np.random.shuffle(x_data)\n",
        "np.random.seed(116)\n",
        "np.random.shuffle(y_data)\n",
        "tf.random.set_seed(116)\n",
        "\n",
        "# 将打乱后的数据集分割为训练集和测试集，训练集为前120行，测试集为后30行\n",
        "x_train = x_data[:-30]\n",
        "y_train = y_data[:-30]\n",
        "x_test = x_data[-30:]\n",
        "y_test = y_data[-30:]\n",
        "\n",
        "# 转换x的数据类型，否则后面矩阵相乘时会因数据类型不一致报错\n",
        "x_train = tf.cast(x_train, tf.float32)\n",
        "x_test = tf.cast(x_test, tf.float32)\n",
        "\n",
        "# from_tensor_slices函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据）\n",
        "train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
        "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
        "\n",
        "# 生成神经网络的参数，4个输入特征故，输入层为4个输入节点；因为3分类，故输出层为3个神经元\n",
        "# 用tf.Variable()标记参数可训练\n",
        "# 使用seed使每次生成的随机数相同（方便教学，使大家结果都一致，在现实使用时不写seed）\n",
        "w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev=0.1, seed=1))\n",
        "b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1))\n",
        "\n",
        "lr = 0.1  # 学习率为0.1\n",
        "train_loss_results = []  # 将每轮的loss记录在此列表中，为后续画loss曲线提供数据\n",
        "test_acc = []  # 将每轮的acc记录在此列表中，为后续画acc曲线提供数据\n",
        "epoch = 500  # 循环500轮\n",
        "loss_all = 0  # 每轮分4个step，loss_all记录四个step生成的4个loss的和\n",
        "\n",
        "# 训练部分\n",
        "now_time = time.time()  ##2##\n",
        "for epoch in range(epoch):  # 数据集级别的循环，每个epoch循环一次数据集\n",
        "    for step, (x_train, y_train) in enumerate(train_db):  # batch级别的循环 ，每个step循环一个batch\n",
        "        with tf.GradientTape() as tape:  # with结构记录梯度信息\n",
        "            y = tf.matmul(x_train, w1) + b1  # 神经网络乘加运算\n",
        "            y = tf.nn.softmax(y)  # 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）\n",
        "            y_ = tf.one_hot(y_train, depth=3)  # 将标签值转换为独热码格式，方便计算loss和accuracy\n",
        "            loss = tf.reduce_mean(tf.square(y_ - y))  # 采用均方误差损失函数mse = mean(sum(y-out)^2)\n",
        "            loss_all += loss.numpy()  # 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确\n",
        "        # 计算loss对各个参数的梯度\n",
        "        grads = tape.gradient(loss, [w1, b1])\n",
        "\n",
        "        # 实现梯度更新 w1 = w1 - lr * w1_grad    b = b - lr * b_grad\n",
        "        w1.assign_sub(lr * grads[0])  # 参数w1自更新\n",
        "        b1.assign_sub(lr * grads[1])  # 参数b自更新\n",
        "\n",
        "    # 每个epoch，打印loss信息\n",
        "    # print(\"Epoch {}, loss: {}\".format(epoch, loss_all / 4))\n",
        "    train_loss_results.append(loss_all / 4)  # 将4个step的loss求平均记录在此变量中\n",
        "    loss_all = 0  # loss_all归零，为记录下一个epoch的loss做准备\n",
        "\n",
        "    # 测试部分\n",
        "    # total_correct为预测对的样本个数, total_number为测试的总样本数，将这两个变量都初始化为0\n",
        "    total_correct, total_number = 0, 0\n",
        "    for x_test, y_test in test_db:\n",
        "        # 使用更新后的参数进行预测\n",
        "        y = tf.matmul(x_test, w1) + b1\n",
        "        y = tf.nn.softmax(y)\n",
        "        pred = tf.argmax(y, axis=1)  # 返回y中最大值的索引，即预测的分类\n",
        "        # 将pred转换为y_test的数据类型\n",
        "        pred = tf.cast(pred, dtype=y_test.dtype)\n",
        "        # 若分类正确，则correct=1，否则为0，将bool型的结果转换为int型\n",
        "        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)\n",
        "        # 将每个batch的correct数加起来\n",
        "        correct = tf.reduce_sum(correct)\n",
        "        # 将所有batch中的correct数加起来\n",
        "        total_correct += int(correct)\n",
        "        # total_number为测试的总样本数，也就是x_test的行数，shape[0]返回变量的行数\n",
        "        total_number += x_test.shape[0]\n",
        "    # 总的准确率等于total_correct/total_number\n",
        "    acc = total_correct / total_number\n",
        "    test_acc.append(acc)\n",
        "    print(\"Test_acc:\", acc)\n",
        "    print(\"--------------------------\")\n",
        "total_time = time.time() - now_time  ##3##\n",
        "print(\"total_time\", total_time)  ##4##\n",
        "\n",
        "# 绘制 loss 曲线\n",
        "plt.title('Loss Function Curve')  # 图片标题\n",
        "plt.xlabel('Epoch')  # x轴变量名称\n",
        "plt.ylabel('Loss')  # y轴变量名称\n",
        "plt.plot(train_loss_results, label=\"$Loss$\")  # 逐点画出trian_loss_results值并连线，连线图标是Loss\n",
        "plt.legend()  # 画出曲线图标\n",
        "plt.show()  # 画出图像\n",
        "\n",
        "# 绘制 Accuracy 曲线\n",
        "plt.title('Acc Curve')  # 图片标题\n",
        "plt.xlabel('Epoch')  # x轴变量名称\n",
        "plt.ylabel('Acc')  # y轴变量名称\n",
        "plt.plot(test_acc, label=\"$Accuracy$\")  # 逐点画出test_acc值并连线，连线图标是Accuracy\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 本文件较 class1\\p45_iris.py 仅添加四处时间记录  用 ##n## 标识\n",
        "# 请将loss曲线、ACC曲线、total_time记录到 class2\\优化器对比.docx  对比各优化器收敛情况\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYrfVmF8s9kv"
      },
      "source": [
        "# 利用鸢尾花数据集，实现前向传播、反向传播，可视化loss曲线\n",
        "\n",
        "# 导入所需模块\n",
        "import tensorflow as tf\n",
        "from sklearn import datasets\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import time  ##1##\n",
        "\n",
        "# 导入数据，分别为输入特征和标签\n",
        "x_data = datasets.load_iris().data\n",
        "y_data = datasets.load_iris().target\n",
        "\n",
        "# 随机打乱数据（因为原始数据是顺序的，顺序不打乱会影响准确率）\n",
        "# seed: 随机数种子，是一个整数，当设置之后，每次生成的随机数都一样（为方便教学，以保每位同学结果一致）\n",
        "np.random.seed(116)  # 使用相同的seed，保证输入特征和标签一一对应\n",
        "np.random.shuffle(x_data)\n",
        "np.random.seed(116)\n",
        "np.random.shuffle(y_data)\n",
        "tf.random.set_seed(116)\n",
        "\n",
        "# 将打乱后的数据集分割为训练集和测试集，训练集为前120行，测试集为后30行\n",
        "x_train = x_data[:-30]\n",
        "y_train = y_data[:-30]\n",
        "x_test = x_data[-30:]\n",
        "y_test = y_data[-30:]\n",
        "\n",
        "# 转换x的数据类型，否则后面矩阵相乘时会因数据类型不一致报错\n",
        "x_train = tf.cast(x_train, tf.float32)\n",
        "x_test = tf.cast(x_test, tf.float32)\n",
        "\n",
        "# from_tensor_slices函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据）\n",
        "train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
        "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
        "\n",
        "# 生成神经网络的参数，4个输入特征故，输入层为4个输入节点；因为3分类，故输出层为3个神经元\n",
        "# 用tf.Variable()标记参数可训练\n",
        "# 使用seed使每次生成的随机数相同（方便教学，使大家结果都一致，在现实使用时不写seed）\n",
        "w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev=0.1, seed=1))\n",
        "b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1))\n",
        "\n",
        "lr = 0.1  # 学习率为0.1\n",
        "train_loss_results = []  # 将每轮的loss记录在此列表中，为后续画loss曲线提供数据\n",
        "test_acc = []  # 将每轮的acc记录在此列表中，为后续画acc曲线提供数据\n",
        "epoch = 500  # 循环500轮\n",
        "loss_all = 0  # 每轮分4个step，loss_all记录四个step生成的4个loss的和\n",
        "\n",
        "##########################################################################\n",
        "v_w, v_b = 0, 0\n",
        "##########################################################################\n",
        "\n",
        "# 训练部分\n",
        "now_time = time.time()  ##2##\n",
        "for epoch in range(epoch):  # 数据集级别的循环，每个epoch循环一次数据集\n",
        "    for step, (x_train, y_train) in enumerate(train_db):  # batch级别的循环 ，每个step循环一个batch\n",
        "        with tf.GradientTape() as tape:  # with结构记录梯度信息\n",
        "            y = tf.matmul(x_train, w1) + b1  # 神经网络乘加运算\n",
        "            y = tf.nn.softmax(y)  # 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）\n",
        "            y_ = tf.one_hot(y_train, depth=3)  # 将标签值转换为独热码格式，方便计算loss和accuracy\n",
        "            loss = tf.reduce_mean(tf.square(y_ - y))  # 采用均方误差损失函数mse = mean(sum(y-out)^2)\n",
        "            loss_all += loss.numpy()  # 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确\n",
        "        # 计算loss对各个参数的梯度\n",
        "        grads = tape.gradient(loss, [w1, b1])\n",
        "\n",
        "        ##########################################################################\n",
        "        # adagrad\n",
        "        v_w += tf.square(grads[0])\n",
        "        v_b += tf.square(grads[1])\n",
        "        w1.assign_sub(lr * grads[0] / tf.sqrt(v_w))\n",
        "        b1.assign_sub(lr * grads[1] / tf.sqrt(v_b))\n",
        "    ##########################################################################\n",
        "\n",
        "    # 每个epoch，打印loss信息\n",
        "    print(\"Epoch {}, loss: {}\".format(epoch, loss_all / 4))\n",
        "    train_loss_results.append(loss_all / 4)  # 将4个step的loss求平均记录在此变量中\n",
        "    loss_all = 0  # loss_all归零，为记录下一个epoch的loss做准备\n",
        "\n",
        "    # 测试部分\n",
        "    # total_correct为预测对的样本个数, total_number为测试的总样本数，将这两个变量都初始化为0\n",
        "    total_correct, total_number = 0, 0\n",
        "    for x_test, y_test in test_db:\n",
        "        # 使用更新后的参数进行预测\n",
        "        y = tf.matmul(x_test, w1) + b1\n",
        "        y = tf.nn.softmax(y)\n",
        "        pred = tf.argmax(y, axis=1)  # 返回y中最大值的索引，即预测的分类\n",
        "        # 将pred转换为y_test的数据类型\n",
        "        pred = tf.cast(pred, dtype=y_test.dtype)\n",
        "        # 若分类正确，则correct=1，否则为0，将bool型的结果转换为int型\n",
        "        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)\n",
        "        # 将每个batch的correct数加起来\n",
        "        correct = tf.reduce_sum(correct)\n",
        "        # 将所有batch中的correct数加起来\n",
        "        total_correct += int(correct)\n",
        "        # total_number为测试的总样本数，也就是x_test的行数，shape[0]返回变量的行数\n",
        "        total_number += x_test.shape[0]\n",
        "    # 总的准确率等于total_correct/total_number\n",
        "    acc = total_correct / total_number\n",
        "    test_acc.append(acc)\n",
        "    print(\"Test_acc:\", acc)\n",
        "    print(\"--------------------------\")\n",
        "total_time = time.time() - now_time  ##3##\n",
        "print(\"total_time\", total_time)  ##4##\n",
        "\n",
        "# 绘制 loss 曲线\n",
        "plt.title('Loss Function Curve')  # 图片标题\n",
        "plt.xlabel('Epoch')  # x轴变量名称\n",
        "plt.ylabel('Loss')  # y轴变量名称\n",
        "plt.plot(train_loss_results, label=\"$Loss$\")  # 逐点画出trian_loss_results值并连线，连线图标是Loss\n",
        "plt.legend()  # 画出曲线图标\n",
        "plt.show()  # 画出图像\n",
        "\n",
        "# 绘制 Accuracy 曲线\n",
        "plt.title('Acc Curve')  # 图片标题\n",
        "plt.xlabel('Epoch')  # x轴变量名称\n",
        "plt.ylabel('Acc')  # y轴变量名称\n",
        "plt.plot(test_acc, label=\"$Accuracy$\")  # 逐点画出test_acc值并连线，连线图标是Accuracy\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 请将loss曲线、ACC曲线、total_time记录到 class2\\优化器对比.docx  对比各优化器收敛情况\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic9Z0bIU44QB"
      },
      "source": [
        "# 利用鸢尾花数据集，实现前向传播、反向传播，可视化loss曲线\n",
        "\n",
        "# 导入所需模块\n",
        "import tensorflow as tf\n",
        "from sklearn import datasets\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import time  ##1##\n",
        "\n",
        "# 导入数据，分别为输入特征和标签\n",
        "x_data = datasets.load_iris().data\n",
        "y_data = datasets.load_iris().target\n",
        "\n",
        "# 随机打乱数据（因为原始数据是顺序的，顺序不打乱会影响准确率）\n",
        "# seed: 随机数种子，是一个整数，当设置之后，每次生成的随机数都一样（为方便教学，以保每位同学结果一致）\n",
        "np.random.seed(116)  # 使用相同的seed，保证输入特征和标签一一对应\n",
        "np.random.shuffle(x_data)\n",
        "np.random.seed(116)\n",
        "np.random.shuffle(y_data)\n",
        "tf.random.set_seed(116)\n",
        "\n",
        "# 将打乱后的数据集分割为训练集和测试集，训练集为前120行，测试集为后30行\n",
        "x_train = x_data[:-30]\n",
        "y_train = y_data[:-30]\n",
        "x_test = x_data[-30:]\n",
        "y_test = y_data[-30:]\n",
        "\n",
        "# 转换x的数据类型，否则后面矩阵相乘时会因数据类型不一致报错\n",
        "x_train = tf.cast(x_train, tf.float32)\n",
        "x_test = tf.cast(x_test, tf.float32)\n",
        "\n",
        "# from_tensor_slices函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据）\n",
        "train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
        "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
        "\n",
        "# 生成神经网络的参数，4个输入特征故，输入层为4个输入节点；因为3分类，故输出层为3个神经元\n",
        "# 用tf.Variable()标记参数可训练\n",
        "# 使用seed使每次生成的随机数相同（方便教学，使大家结果都一致，在现实使用时不写seed）\n",
        "w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev=0.1, seed=1))\n",
        "b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1))\n",
        "\n",
        "lr = 0.1  # 学习率为0.1\n",
        "train_loss_results = []  # 将每轮的loss记录在此列表中，为后续画loss曲线提供数据\n",
        "test_acc = []  # 将每轮的acc记录在此列表中，为后续画acc曲线提供数据\n",
        "epoch = 500  # 循环500轮\n",
        "loss_all = 0  # 每轮分4个step，loss_all记录四个step生成的4个loss的和\n",
        "\n",
        "##########################################################################\n",
        "v_w, v_b = 0, 0\n",
        "beta = 0.9\n",
        "##########################################################################\n",
        "\n",
        "# 训练部分\n",
        "now_time = time.time()  ##2##\n",
        "for epoch in range(epoch):  # 数据集级别的循环，每个epoch循环一次数据集\n",
        "    for step, (x_train, y_train) in enumerate(train_db):  # batch级别的循环 ，每个step循环一个batch\n",
        "        with tf.GradientTape() as tape:  # with结构记录梯度信息\n",
        "            y = tf.matmul(x_train, w1) + b1  # 神经网络乘加运算\n",
        "            y = tf.nn.softmax(y)  # 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）\n",
        "            y_ = tf.one_hot(y_train, depth=3)  # 将标签值转换为独热码格式，方便计算loss和accuracy\n",
        "            loss = tf.reduce_mean(tf.square(y_ - y))  # 采用均方误差损失函数mse = mean(sum(y-out)^2)\n",
        "            loss_all += loss.numpy()  # 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确\n",
        "        # 计算loss对各个参数的梯度\n",
        "        grads = tape.gradient(loss, [w1, b1])\n",
        "\n",
        "        ##########################################################################\n",
        "        # rmsprop\n",
        "        v_w = beta * v_w + (1 - beta) * tf.square(grads[0])\n",
        "        v_b = beta * v_b + (1 - beta) * tf.square(grads[1])\n",
        "        w1.assign_sub(lr * grads[0] / tf.sqrt(v_w))\n",
        "        b1.assign_sub(lr * grads[1] / tf.sqrt(v_b))\n",
        "    ##########################################################################\n",
        "\n",
        "    # 每个epoch，打印loss信息\n",
        "    print(\"Epoch {}, loss: {}\".format(epoch, loss_all / 4))\n",
        "    train_loss_results.append(loss_all / 4)  # 将4个step的loss求平均记录在此变量中\n",
        "    loss_all = 0  # loss_all归零，为记录下一个epoch的loss做准备\n",
        "\n",
        "    # 测试部分\n",
        "    # total_correct为预测对的样本个数, total_number为测试的总样本数，将这两个变量都初始化为0\n",
        "    total_correct, total_number = 0, 0\n",
        "    for x_test, y_test in test_db:\n",
        "        # 使用更新后的参数进行预测\n",
        "        y = tf.matmul(x_test, w1) + b1\n",
        "        y = tf.nn.softmax(y)\n",
        "        pred = tf.argmax(y, axis=1)  # 返回y中最大值的索引，即预测的分类\n",
        "        # 将pred转换为y_test的数据类型\n",
        "        pred = tf.cast(pred, dtype=y_test.dtype)\n",
        "        # 若分类正确，则correct=1，否则为0，将bool型的结果转换为int型\n",
        "        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)\n",
        "        # 将每个batch的correct数加起来\n",
        "        correct = tf.reduce_sum(correct)\n",
        "        # 将所有batch中的correct数加起来\n",
        "        total_correct += int(correct)\n",
        "        # total_number为测试的总样本数，也就是x_test的行数，shape[0]返回变量的行数\n",
        "        total_number += x_test.shape[0]\n",
        "    # 总的准确率等于total_correct/total_number\n",
        "    acc = total_correct / total_number\n",
        "    test_acc.append(acc)\n",
        "    print(\"Test_acc:\", acc)\n",
        "    print(\"--------------------------\")\n",
        "total_time = time.time() - now_time  ##3##\n",
        "print(\"total_time\", total_time)  ##4##\n",
        "\n",
        "# 绘制 loss 曲线\n",
        "plt.title('Loss Function Curve')  # 图片标题\n",
        "plt.xlabel('Epoch')  # x轴变量名称\n",
        "plt.ylabel('Loss')  # y轴变量名称\n",
        "plt.plot(train_loss_results, label=\"$Loss$\")  # 逐点画出trian_loss_results值并连线，连线图标是Loss\n",
        "plt.legend()  # 画出曲线图标\n",
        "plt.show()  # 画出图像\n",
        "\n",
        "# 绘制 Accuracy 曲线\n",
        "plt.title('Acc Curve')  # 图片标题\n",
        "plt.xlabel('Epoch')  # x轴变量名称\n",
        "plt.ylabel('Acc')  # y轴变量名称\n",
        "plt.plot(test_acc, label=\"$Accuracy$\")  # 逐点画出test_acc值并连线，连线图标是Accuracy\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 请将loss曲线、ACC曲线、total_time记录到 class2\\优化器对比.docx  对比各优化器收敛情况\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuzAn_9N7m7q"
      },
      "source": [
        "# 利用鸢尾花数据集，实现前向传播、反向传播，可视化loss曲线\n",
        "\n",
        "# 导入所需模块\n",
        "import tensorflow as tf\n",
        "from sklearn import datasets\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import time  ##1##\n",
        "\n",
        "# 导入数据，分别为输入特征和标签\n",
        "x_data = datasets.load_iris().data\n",
        "y_data = datasets.load_iris().target\n",
        "\n",
        "# 随机打乱数据（因为原始数据是顺序的，顺序不打乱会影响准确率）\n",
        "# seed: 随机数种子，是一个整数，当设置之后，每次生成的随机数都一样（为方便教学，以保每位同学结果一致）\n",
        "np.random.seed(116)  # 使用相同的seed，保证输入特征和标签一一对应\n",
        "np.random.shuffle(x_data)\n",
        "np.random.seed(116)\n",
        "np.random.shuffle(y_data)\n",
        "tf.random.set_seed(116)\n",
        "\n",
        "# 将打乱后的数据集分割为训练集和测试集，训练集为前120行，测试集为后30行\n",
        "x_train = x_data[:-30]\n",
        "y_train = y_data[:-30]\n",
        "x_test = x_data[-30:]\n",
        "y_test = y_data[-30:]\n",
        "\n",
        "# 转换x的数据类型，否则后面矩阵相乘时会因数据类型不一致报错\n",
        "x_train = tf.cast(x_train, tf.float32)\n",
        "x_test = tf.cast(x_test, tf.float32)\n",
        "\n",
        "# from_tensor_slices函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据）\n",
        "train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
        "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
        "\n",
        "# 生成神经网络的参数，4个输入特征故，输入层为4个输入节点；因为3分类，故输出层为3个神经元\n",
        "# 用tf.Variable()标记参数可训练\n",
        "# 使用seed使每次生成的随机数相同（方便教学，使大家结果都一致，在现实使用时不写seed）\n",
        "w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev=0.1, seed=1))\n",
        "b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1))\n",
        "\n",
        "lr = 0.1  # 学习率为0.1\n",
        "train_loss_results = []  # 将每轮的loss记录在此列表中，为后续画loss曲线提供数据\n",
        "test_acc = []  # 将每轮的acc记录在此列表中，为后续画acc曲线提供数据\n",
        "epoch = 500  # 循环500轮\n",
        "loss_all = 0  # 每轮分4个step，loss_all记录四个step生成的4个loss的和\n",
        "\n",
        "##########################################################################\n",
        "m_w, m_b = 0, 0\n",
        "v_w, v_b = 0, 0\n",
        "beta1, beta2 = 0.9, 0.999\n",
        "delta_w, delta_b = 0, 0\n",
        "global_step = 0\n",
        "##########################################################################\n",
        "\n",
        "# 训练部分\n",
        "now_time = time.time()  ##2##\n",
        "for epoch in range(epoch):  # 数据集级别的循环，每个epoch循环一次数据集\n",
        "    for step, (x_train, y_train) in enumerate(train_db):  # batch级别的循环 ，每个step循环一个batch\n",
        " ##########################################################################       \n",
        "        global_step += 1\n",
        " ##########################################################################       \n",
        "        with tf.GradientTape() as tape:  # with结构记录梯度信息\n",
        "            y = tf.matmul(x_train, w1) + b1  # 神经网络乘加运算\n",
        "            y = tf.nn.softmax(y)  # 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）\n",
        "            y_ = tf.one_hot(y_train, depth=3)  # 将标签值转换为独热码格式，方便计算loss和accuracy\n",
        "            loss = tf.reduce_mean(tf.square(y_ - y))  # 采用均方误差损失函数mse = mean(sum(y-out)^2)\n",
        "            loss_all += loss.numpy()  # 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确\n",
        "        # 计算loss对各个参数的梯度\n",
        "        grads = tape.gradient(loss, [w1, b1])\n",
        "\n",
        "##########################################################################\n",
        " # adam\n",
        "        m_w = beta1 * m_w + (1 - beta1) * grads[0]\n",
        "        m_b = beta1 * m_b + (1 - beta1) * grads[1]\n",
        "        v_w = beta2 * v_w + (1 - beta2) * tf.square(grads[0])\n",
        "        v_b = beta2 * v_b + (1 - beta2) * tf.square(grads[1])\n",
        "\n",
        "        m_w_correction = m_w / (1 - tf.pow(beta1, int(global_step)))\n",
        "        m_b_correction = m_b / (1 - tf.pow(beta1, int(global_step)))\n",
        "        v_w_correction = v_w / (1 - tf.pow(beta2, int(global_step)))\n",
        "        v_b_correction = v_b / (1 - tf.pow(beta2, int(global_step)))\n",
        "\n",
        "        w1.assign_sub(lr * m_w_correction / tf.sqrt(v_w_correction))\n",
        "        b1.assign_sub(lr * m_b_correction / tf.sqrt(v_b_correction))\n",
        "##########################################################################\n",
        "\n",
        "    # 每个epoch，打印loss信息\n",
        "    print(\"Epoch {}, loss: {}\".format(epoch, loss_all / 4))\n",
        "    train_loss_results.append(loss_all / 4)  # 将4个step的loss求平均记录在此变量中\n",
        "    loss_all = 0  # loss_all归零，为记录下一个epoch的loss做准备\n",
        "\n",
        "    # 测试部分\n",
        "    # total_correct为预测对的样本个数, total_number为测试的总样本数，将这两个变量都初始化为0\n",
        "    total_correct, total_number = 0, 0\n",
        "    # for x_test, y_test in test_db:\n",
        "    #     # 使用更新后的参数进行预测\n",
        "    #     y = tf.matmul(x_test, w1) + b1\n",
        "    #     y = tf.nn.softmax(y)\n",
        "    #     pred = tf.argmax(y, axis=1)  # 返回y中最大值的索引，即预测的分类\n",
        "    #     # 将pred转换为y_test的数据类型\n",
        "    #     pred = tf.cast(pred, dtype=y_test.dtype)\n",
        "    #     # 若分类正确，则correct=1，否则为0，将bool型的结果转换为int型\n",
        "    #     correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)\n",
        "    #     # 将每个batch的correct数加起来\n",
        "    #     correct = tf.reduce_sum(correct)\n",
        "    #     # 将所有batch中的correct数加起来\n",
        "    #     total_correct += int(correct)\n",
        "    #     # total_number为测试的总样本数，也就是x_test的行数，shape[0]返回变量的行数\n",
        "    #     total_number += x_test.shape[0]\n",
        "    # # 总的准确率等于total_correct/total_number\n",
        "    # acc = total_correct / total_number\n",
        "    # test_acc.append(acc)\n",
        "    # print(\"Test_acc:\", acc)\n",
        "    # print(\"--------------------------\")\n",
        "total_time = time.time() - now_time  ##3##\n",
        "print(\"total_time\", total_time)  ##4##\n",
        "\n",
        "# 绘制 loss 曲线\n",
        "plt.title('Loss Function Curve')  # 图片标题\n",
        "plt.xlabel('Epoch')  # x轴变量名称\n",
        "plt.ylabel('Loss')  # y轴变量名称\n",
        "plt.plot(train_loss_results, label=\"$Loss$\")  # 逐点画出trian_loss_results值并连线，连线图标是Loss\n",
        "plt.legend()  # 画出曲线图标\n",
        "plt.show()  # 画出图像\n",
        "\n",
        "# # 绘制 Accuracy 曲线\n",
        "# plt.title('Acc Curve')  # 图片标题\n",
        "# plt.xlabel('Epoch')  # x轴变量名称\n",
        "# plt.ylabel('Acc')  # y轴变量名称\n",
        "# plt.plot(test_acc, label=\"$Accuracy$\")  # 逐点画出test_acc值并连线，连线图标是Accuracy\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# 请将loss曲线、ACC曲线、total_time记录到 class2\\优化器对比.docx  对比各优化器收敛情况\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-QZ3WGtBl-B"
      },
      "source": [
        "print(tf.pow(tf.fill([1,2],3.),3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWjRQ9rNEqVb"
      },
      "source": [
        "print(tf.sqrt(tf.fill([1,2],3.),3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfRGAnl2E3nY"
      },
      "source": [
        "features=tf.constant([12,23,13,22])\n",
        "lable=tf.constant([1,0,0,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7lKY7yNFXFZ"
      },
      "source": [
        "dataset=tf.data.Dataset.from_tensor_slices((features,lable))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFyuNAN_FirC"
      },
      "source": [
        "print(dataset)\n",
        "for ele in dataset:\n",
        "  print(ele)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47xE_9n_Fk-H"
      },
      "source": [
        "with tf.GradientTape() as tape:\n",
        "  x=tf.Variable(tf.constant(3.0))\n",
        "  y=tf.pow(x,3)\n",
        "grad=tape.gradient(y,x)\n",
        "print(grad)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zinG04m-GZxP"
      },
      "source": [
        "for i ,ele in enumerate(dataset):\n",
        "  print(i,ele)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWVyj0wVH4A8"
      },
      "source": [
        "tf.one_hot(tf.constant([1,3,2]),depth=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBaW6-AbIT6Z"
      },
      "source": [
        "x1 = tf.constant([[5.8, 4.0, 1.2, 0.2]])  # 5.8,4.0,1.2,0.2（0）\n",
        "w1 = tf.constant([[-0.8, -0.34, -1.4],\n",
        "                  [0.6, 1.3, 0.25],\n",
        "                  [0.5, 1.45, 0.9],\n",
        "                  [0.65, 0.7, -1.2]])\n",
        "b1 = tf.constant([2.52, -3.1, 5.62])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hafU8ns_KnBm"
      },
      "source": [
        "tf.constant([5.8, 4.0, 1.2, 0.2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlNl09dXKiAE"
      },
      "source": [
        "x1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Uya8RsnJcIF"
      },
      "source": [
        "w1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2Pif9L6JeO0"
      },
      "source": [
        "y=tf.matmul(x1,w1)+b1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEgcSDDDJqc0"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB9YejqEJrM6"
      },
      "source": [
        "y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRqa8anKJ1Y3"
      },
      "source": [
        "y_dim=tf.squeeze(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw11-5pgKE3F"
      },
      "source": [
        "tf.nn.softmax(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S84kk8ffKyIN"
      },
      "source": [
        "tf.nn.softmax(y_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4ZCzf7OK-TB"
      },
      "source": [
        "x=tf.Variable(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7E7KeSHLXHI"
      },
      "source": [
        "x.assign(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5_kzVBOLY_l"
      },
      "source": [
        "x.assign_sub(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HyJFt-cLdQ5"
      },
      "source": [
        "tf.argmax(w1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhfXbQNLLm9U"
      },
      "source": [
        "from sklearn import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qxwwPIMMZU4"
      },
      "source": [
        "from pandas import DataFrame\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhYLAZIpMf1h"
      },
      "source": [
        "\n",
        "x_data = datasets.load_iris().data  # .data返回iris数据集所有输入特征\n",
        "y_data = datasets.load_iris().target  # .target返回iris数据集所有标签\n",
        "print(\"x_data from datasets: \\n\", x_data)\n",
        "print(\"y_data from datasets: \\n\", y_data)\n",
        "\n",
        "x_data = DataFrame(x_data, columns=['花萼长度', '花萼宽度', '花瓣长度', '花瓣宽度']) # 为表格增加行索引（左侧）和列标签（上方）\n",
        "pd.set_option('display.unicode.east_asian_width', True)  # 设置列名对齐\n",
        "print(\"x_data add index: \\n\", x_data)\n",
        "\n",
        "x_data['类别'] = y_data  # 新加一列，列标签为‘类别’，数据为y_data\n",
        "print(\"x_data add a column: \\n\", x_data)\n",
        "\n",
        "#类型维度不确定时，建议用print函数打印出来确认效果"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k-CNQ3iMoCW"
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn import datasets\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7iPJDON1vmB"
      },
      "source": [
        "iris=datasets.load_iris()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkuWVRVZ15qS"
      },
      "source": [
        "x_data=iris.data\n",
        "y_data=iris.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWE8u4Yq2Ixe"
      },
      "source": [
        "# feature and lable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDa_jqDR2KtO"
      },
      "source": [
        "np.random.seed(1)\n",
        "np.random.shuffle(x_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmCbW47B202Q"
      },
      "source": [
        "np.random.seed(1)\n",
        "np.random.shuffle(y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1izGbJgS26Yz"
      },
      "source": [
        "x_train=x_data[:-30]\n",
        "y_train=y_data[:-30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g01A4b2g2_A4"
      },
      "source": [
        "x_test=x_data[-30:]\n",
        "y_test=y_data[-30:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKDEK0tT3gft"
      },
      "source": [
        "# change dataType"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INHKR9xR3ovD"
      },
      "source": [
        "x_train=tf.cast(x_train,tf.float32)\n",
        "x_test=tf.cast(x_test,tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-12hMNYT34Rc"
      },
      "source": [
        "train_db=tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
        "test_db=tf.data.Dataset.from_tensor_slices((x_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWGxMrVS4g3h"
      },
      "source": [
        "train_db=train_db.batch(30)\n",
        "test_db=test_db.batch(30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09Uapajy4zvi"
      },
      "source": [
        "x_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aLIgNma75iD"
      },
      "source": [
        "y_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMrAymke77GP"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxfeW33n7MnH"
      },
      "source": [
        "model=tf.keras.models.Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpmviBvi7Zlm"
      },
      "source": [
        "model.fit(verbose=)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvthKb2BrsE6"
      },
      "source": [
        "!git clone https://github.com/BLarzalere/LSTM-Autoencoder-for-Anomaly-Detection.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceXJxa0IrvG6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}